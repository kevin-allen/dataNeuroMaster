{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science in Neuroscience\n",
    "\n",
    "\n",
    "## Plan for today\n",
    "\n",
    "1. Review of last week's exercises\n",
    "2. Why and how are in-vivo recordings performed?\n",
    "3. Detecting action potentials and spike clustering\n",
    "    * Filtering\n",
    "    * Detecting spikes\n",
    "    * Extracting spike waveforms\n",
    "    * Dimensionality reduction\n",
    "    * Clustering spike waveforms\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of last week's exercises\n",
    "\n",
    "Reload the data from last week.\n",
    "\n",
    "The is a recording from 8 electrodes/channels for 1 second at 20 000 Hz.\n",
    "\n",
    "\n",
    "After completing the exercises, you should now know that the array looks like the one in this figure.\n",
    "<div>\n",
    "<img src=\"../images/array.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fileName = \"../data/shortRaw.npy\" # binder users or people with a local git repository\n",
    "dat = np.load(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What do you think the electrodes are measuring? \n",
    "* How many dimensions does the data have?\n",
    "* What is the size of the array in each dimension?\n",
    "* What is the data type or dtype (integer, float, etc.) of the `dat` array?\n",
    "* Which dimension represents the recording channels and the recording samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot the data for the first recording channel.\n",
    "* Do you seen any oscillatory patterns in the data? If so at which frequency? \n",
    "* Plot the data for 2 recording channels. \n",
    "* What is similar or different between the different channels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What is the mean value in the `dat` array?\n",
    "* What is the mean value per channel of the `dat` array?\n",
    "* Can you plot the mean value across channels for all samples? The plot should have 20000 values.\n",
    "* How could I add 100 to all values in the array?\n",
    "* Which channel has the largest standard deviation?\n",
    "* Can you plot the distribution of values for the second recording channel?\n",
    "* For each channel, remove the mean from every data point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Why and how are in-vivo recordings performed?\n",
    "\n",
    "**Aim:** Monitor the activity of groups of neurons during behavior. \n",
    "\n",
    "The correlations between cell activity and behavior informs us about the potential role of a brain area for behavior. We can learn how information is processed within neuronal circuits. The effect of different manipulations (optogenetics, pharmacology) on network activity can be assessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Spatially selective neurons in the medial entorhinal cortex (MEC)\n",
    "\n",
    "By recording the activity of neurons in the entorhinal cortex in freely-moving mice, scientists discovered that MEC neurons encode where the animal is in its environment and in which direction it is heading. This brain region is essential for navigation.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/mec_neurons.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "* Right column: Spikes (red) on top of the path of the animal (black).\n",
    "* Left column: Firing rate of the neuron as a function of position (top), head-direction (middle) and running speed (bottom). \n",
    "\n",
    "\n",
    "### References\n",
    "* Hafting et al., 2005. Microstructure of a spatial map in the entorhinal cortex. Nature\n",
    "* Sargolini et al., 2006. Conjunctive Representation of Position, Direction, and Velocity in Entorhinal Cortex. Nature\n",
    "* Gil et al., 2018. Impaired path integration in mice with disrupted grid cell firing. Nature Neuroscience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Electrodes -> Amplifier -> Digitalization -> Recording on disk\n",
    "\n",
    "Electrodes located in the extracellular space can pick up action potentials (sharp negative fluctuation in voltage) from several neurons. \n",
    "\n",
    "<div>\n",
    "<img src=\"../images/buzsaki_tetrode_in_layer.jpg\" width=\"600\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"../images/buzsaki_probe.webp\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/intan_64.jpg\" width=\"300\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "* Buzsaki, 2004. Large-scale recording of neuronal ensembles. Nature Neuroscience\n",
    "* Steinmetz et al., 2021. Neuropixels 2.0: A miniaturized high-density probe for stable, long-term brain recordings. Science\n",
    "* https://intantech.com/\n",
    "\n",
    "\n",
    "The recording system sample the voltage on multiple channels at 20-30 kHz. The amplified signal is saved to disk for later analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# Spike extraction and clustering\n",
    "\n",
    "We want to study the activity (action potentials) of single neurons. We need to find at which time which neuron fired action potentials. \n",
    "\n",
    "One assumption is that the all spikes from one neuron will have a constant waveform on the different channels. The pattern across the different channels depend on the proximity of the neuron to the different electrodes (channels).\n",
    "\n",
    "Let's have a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset=3000 #y-shift between channels when plotting the data\n",
    "nSamples=5000\n",
    "plt.figure(figsize=(20,10)) # set the size of the figure\n",
    "\n",
    "for i in range(dat.shape[0]): #loop for every channel\n",
    "    plt.plot(dat[i,:nSamples]-i*3000,label=i) # plot the channel, apply the y-offset so that the channels are not all on top of each other\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps involved\n",
    "* Filtering\n",
    "* Identifying spikes\n",
    "* Extracting spike waveforms\n",
    "* Dimensionality reduction\n",
    "* Clustering spike waveforms\n",
    "\n",
    "Note: In the lab, you would normally use an established software package to do this (Kilosort, Spyking circus, etc.).\n",
    "\n",
    "Many classic data processing steps are involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "## Filtering\n",
    "\n",
    "The signal recorded is the sum of several components: \n",
    "\n",
    "1. Large amplitude spikes of a few neurons close to the recording sites\n",
    "2. Small amplitude spikes of many distant neurons\n",
    "3. Fluctuations in synaptic currents onto thousands of neurons (part of local field potentials)\n",
    "4. Electrical noise from electrical equipment (usually 50 Hz)\n",
    "5. Head muscle contractions (e.g., noise associated with chewing)\n",
    "6. Motion artifacts\n",
    "\n",
    "If a neuron is close enough to our electrodes, its action potentials create rapid (usually) negative deflections in the signal. The spikes in extracellular space last less than 1 ms.\n",
    "\n",
    "To extract action potentials, we need to separate these fast negative deflections from other slower components of the signal. That is why we need to filter the signal.\n",
    "\n",
    "[https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html](https://docs.scipy.org/doc/scipy/reference/tutorial/signal.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Butterworth filter\n",
    "\n",
    "We eliminate/reduce low-frequency oscillations (below 300 Hz) and keep faster oscillations (including spikes). This is called a highpass filter (as opposed to lowpass and bandpass).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import butter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 20000 # sampling rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_butter_highpass_filter(lowcut, fs, order=3):\n",
    "    \"\"\"\n",
    "    Create a Butterworth filter using scipy.signal.butter()\n",
    "    \"\"\"\n",
    "    nyq = 0.5 * fs # Nyquist frequency (sampling rate/2)\n",
    "    low = lowcut / nyq # lowcut as a proportion of Nyquist frequency\n",
    "    sos = butter(order, [low], btype='highpass' ,  output='sos') #‘sos’ should be used for general-purpose filtering.\n",
    "    return sos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowcut=300\n",
    "myFilter = create_butter_highpass_filter(lowcut = lowcut,fs=fs, order=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the frequency response of the filter\n",
    "\n",
    "It should be near 0 and flat at the frequencies that you want to keep, and below 0 for frequencies that you want to filter out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the frequency response of our filter for visualization\n",
    "from scipy.signal import sosfreqz\n",
    "w, h = sosfreqz(myFilter, worN=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = 20*np.log10(np.maximum(np.abs(h), 1e-5)) # don't worry about this, transforms h to dB unit\n",
    "plt.plot(w/np.pi*fs/2, db) # first part transform w to in Hz unit\n",
    "plt.ylim(-75, 5)\n",
    "plt.ylabel('Gain (dB)')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.title('Frequency Response')\n",
    "plt.plot([lowcut,lowcut],[-75,0],c=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the fitler to a recording channel\n",
    "\n",
    "We can apply our filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import sosfiltfilt # see https://stackoverflow.com/questions/12093594/how-to-implement-band-pass-butterworth-filter-with-scipy-signal-butter\n",
    "chan=3\n",
    "y = sosfiltfilt(myFilter, dat[chan,:]) # we filter the raw data from channel 3 and save the results in y\n",
    "print(y.shape, dat[chan,:].shape) # y has same shape as the raw data from channel 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the raw and filtered signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSamples=20000\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(dat[chan,0:nSamples])\n",
    "plt.plot(y[0:nSamples]-3000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot fewer data points to zoom in on the raw and filtered signals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSamples=2000\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(dat[chan,:0+nSamples])\n",
    "plt.plot(y[:0+nSamples]-2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: \n",
    "\n",
    "* Try to apply the filter to the 8 recording channels. \n",
    "* How could you store/save the filtered data for each channel.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, don't just assume that the filter was applied to all channels, plot the data to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset=3000 #y shift between channels\n",
    "nSamples=20000\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(y.shape[0]): #loop for every channel\n",
    "    plt.plot(y[i,:nSamples]-i*3000,label=i) # plot the filtered data\n",
    "    plt.plot(dat[i,:nSamples]-i*3000,c=\"gray\",alpha=0.25) # plot original signal in transparent gray\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Detecting spikes\n",
    "\n",
    "We have removed slow owcillations from the raw signal. We can try to identify spikes in the filtered signal.\n",
    "\n",
    "One simple strategy is to calculate the **mean** and **standard deviation** of the filtered signal. A spike occures each time the signal reaches 5 standard deviation below the mean. This is the __spike detection threshold__.\n",
    "\n",
    "\n",
    "Steps:\n",
    "\n",
    "* Calculate mean and standard deviation\n",
    "* Calculate the spike detection threshold\n",
    "* Detect the events that reached the spike detection threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the mean and standard deviation\n",
    "\n",
    "We will use a single channel (chan=3) to simplify the problem.\n",
    "\n",
    "\n",
    "#### Exercise\n",
    "\n",
    "* Calculate and print the mean and standard deviation of channel indexed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the spike detection threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=5 # 5 std below the mean\n",
    "threshold = mean-std*z\n",
    "print(\"threshold:\",threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data to check if this makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(y[chan,:],label=\"Filtered data\")\n",
    "plt.plot([0,y.shape[1]],[mean,mean], label=\"Mean\")\n",
    "plt.plot([0,y.shape[1]],[threshold,threshold], label=\"Spike detection threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can zoom in by plotting only the first 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSamples=1000\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(y[chan,:nSamples],label=\"Filtered data\")\n",
    "plt.plot([0,nSamples],[mean,mean],label=\"Mean\")\n",
    "plt.plot([0,nSamples],[threshold,threshold],label=\"Spike detection threshold\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect events that reached the spike detection threshold\n",
    "\n",
    "We now need to find the trough of the detected spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one way to do it:\n",
    "\n",
    "* Inverse the data so that the spikes are positive\n",
    "* Detect the local peaks in the data using scipy.signal.find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nSamples=2000\n",
    "\n",
    "## important point about np.arrays\n",
    "yDetect = y[chan,:].copy() # use a copy of the data to avoid destroying it during the manipulation\n",
    "\n",
    "plt.plot(yDetect[0:nSamples])\n",
    "plt.show()\n",
    "\n",
    "# inverse the signal to get positive peaks\n",
    "yDetect = 0 - yDetect # inverse \n",
    "plt.plot(yDetect[0:nSamples])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`scipy.signal.find_peaks`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html) can identify the time of the peaks (spikes) for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "spikeTimes, _ = find_peaks(x=yDetect,height=0-threshold) # the function returs 2 values, we want the first one\n",
    "print(\"Number of detected spikes:\",spikeTimes.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the filtered signal with the detected spike times. Always plot the results to make sure you get what you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chan=3 # select a channel to work with\n",
    "nSamples=3000\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(y[chan,:nSamples],label=\"Filtered signal\")\n",
    "plt.scatter(spikeTimes,y[chan,spikeTimes],c=\"red\",label=\"Spike times\")\n",
    "plt.plot([0,nSamples],[threshold,threshold],label=\"Spike-detection threshold\") \n",
    "plt.xlim(0,nSamples)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative spikes crossing the spike-detection threshold are identified correctly by the find_peaks() function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises (in class or outside depending on time)\n",
    "\n",
    "1. Write the code that would apply our spike detection procedure to the 8 recording channels in our data? \n",
    "    a. You could try to use a `for` loops to loop across the channels. Within this loop, you can reuse the code that we used below.\n",
    "    b. You could store the spike times array in a list of array.\n",
    "2. Try to plot the data from all channels with their threshold and detected spikes. This would confirm that your spike detection worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Extracting spike waveforms\n",
    "\n",
    "We have detected 23 spikes on channel 3. In this steps, we want to extract the waveforms for these spikes from the filtered signal and store it in a new NumPy array.\n",
    "\n",
    "<div>\n",
    "<img src=\"../images/spike_extraction.png\" width=\"800\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we need to decide on how many samples we collect per spike. 40 could be a reasonable choice. This is 1 ms before and 1 ms after the spike negative peak. \n",
    "\n",
    "\n",
    "* Axis 0: 8 channels\n",
    "* Axis 1: 40 samples\n",
    "* Axis 2: 23 spikes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our 3D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplesPerSpike=40\n",
    "waveforms=np.empty((dat.shape[0],samplesPerSpike,spikeTimes.shape[0]))\n",
    "print(\"Shape of the waveforms array:\",waveforms.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract a first spike to test our strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = spikeTimes[0]\n",
    "print(st)\n",
    "start_index=int(st-samplesPerSpike/2)\n",
    "end_index=int(st+samplesPerSpike/2)\n",
    "print(start_index,end_index)\n",
    "waveforms[:,:,0] = y[:,start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, plot your results to make sure it looks like you expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(waveforms[:,:,0])\n",
    "plt.show()\n",
    "plt.plot(waveforms[3,:,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all 23 spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,st in enumerate(spikeTimes):\n",
    "    start_index=int(st-samplesPerSpike/2)\n",
    "    end_index=int(st+samplesPerSpike/2)\n",
    "    waveforms[:,:,i] = y[:,start_index:end_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, **visualize your results**. As 3D plots are hard to visualize, we will plot a series of 2D plots.\n",
    "\n",
    "Here are some guidlines to visualize the content of NumPy arrays of different dimensions. \n",
    "\n",
    "#### 1D \n",
    "* plt.plot(x)\n",
    "* plt.hist(x)\n",
    "\n",
    "#### 2D\n",
    "* plt.plot(x[i,:]\n",
    "* plt.hist(x[i,:]))\n",
    "* plt.imshow()\n",
    "\n",
    "#### 3D\n",
    "* plt.plot(x[i,j,:])\n",
    "* plt.imshow(x[i,:,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows=5\n",
    "ncols=5\n",
    "fig, ax = plt.subplots(5,5,figsize=(15,18))\n",
    "for i in range(waveforms.shape[2]):\n",
    "    #print(i,int(i/5),i%5)\n",
    "    ax[int(i/5),i%5].imshow(waveforms[:,:,i],aspect=\"auto\")\n",
    "    ax[int(i/5),i%5].set_title(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "* What do you see in this figure?\n",
    "* Do you notice some patterns in the data?\n",
    "* Are there spikes that are clearly different from the other ones?\n",
    "* Do you think there are groups of spikes with similar waveforms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Dimensionality reduction\n",
    "\n",
    "Our end goal is to cluster the spikes in groups with very similar spike waveforms (putative neurons). This can be done with different clustering algorithms. In the context of clustering, every data point of a 2D waveform is a feature of the waveform.\n",
    "\n",
    "One problem is that we have 320 (8 channels * 40 samples) data points per spikes. If we had more than 100,000 spikes as commonly seen in the lab, the clustering algorithms would be very slow.\n",
    "\n",
    "Our aim with dimensionality reduction is to reduce the number of features to a lower number.\n",
    "\n",
    "The traditional way to do this is with **principal component analysis**. \n",
    "\n",
    "PS: There are now fancier way to do dimensionality reduction (e.g., autoencoder).\n",
    "\n",
    "### Principal component analysis\n",
    "\n",
    "Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.\n",
    "\n",
    "Each component is a linear combination of the features and a set of coefficients. The first component is the vector in which there is the most variance in the data.\n",
    "\n",
    "* [wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
    "* [PCA with scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n",
    "\n",
    "We will use a class from the [scikit-learn](https://scikit-learn.org/stable/index.html#) package to do PCA. scikit-learn is a package to apply machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=15) # the number of components should be lower than number of spikes (usually not a problem!)\n",
    "type(pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use PCA, we need to get a 2D array (spikes,features) from our 3D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(waveforms.shape)\n",
    "wf_fea = waveforms.transpose(2,0,1)\n",
    "print(wf_fea.shape)\n",
    "wf_fea = wf_fea.reshape(wf_fea.shape[0],-1)\n",
    "print(\"dimension:\",wf_fea.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(wf_fea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print(\"Total explained variance: \", np.sum(pca.explained_variance_ratio_))\n",
    "#\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.ylim(0,1)\n",
    "plt.xlabel(\"Components\")\n",
    "plt.ylabel(\"Explained variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_fea_pca = pca.transform(wf_fea)\n",
    "print(\"new dimensions:\",wf_fea_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went from 320 to 15 features per spikes. \n",
    "\n",
    "We can print the first 2 PC of the spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(wf_fea_pca[:,0],wf_fea_pca[:,1])\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was there a spike that looked very different than the other one? One spike is far away from the other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PCA 1:\",wf_fea_pca[:,0])\n",
    "print(\"Spike with the largest PCA1: \", np.argmax(wf_fea_pca[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
